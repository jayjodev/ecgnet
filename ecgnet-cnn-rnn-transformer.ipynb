{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3531470,"sourceType":"datasetVersion","datasetId":2124270}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport wfdb\nimport pywt\nfrom scipy.signal import butter, filtfilt\n\n# 데이터 로딩 및 전처리 함수\ndef load_and_preprocess_ecg_data(base_path):\n    all_segments = []\n    all_labels = []\n\n    for person_id in range(1, 90):  # Person_01부터 Person_89까지\n        person_path = os.path.join(base_path, f'Person_{person_id:02}')\n        if not os.path.exists(person_path):\n            print(f\"Directory {person_path} does not exist. Skipping.\")\n            continue\n        for filename in os.listdir(person_path):\n            if filename.endswith('.hea') and not filename.startswith('.'):\n                record_name = filename.split('.')[0]\n                record_path = os.path.join(person_path, record_name)\n                try:\n                    record = wfdb.rdrecord(record_path)\n                    annotation = wfdb.rdann(record_path, 'atr')\n                    ecg_signal = record.p_signal[:, 0]\n                    ecg_signal = remove_baseline_drift(ecg_signal)\n                    ecg_signal = bandpass_filter(ecg_signal)\n                    segments = normalize_and_segment(ecg_signal, annotation.sample)\n                    all_segments.extend(segments)\n                    all_labels.extend([person_id - 1] * len(segments))\n                except Exception as e:\n                    print(f\"Error processing {record_path}: {e}\")\n                    continue\n\n    return np.array(all_segments), np.array(all_labels)\n\ndef remove_baseline_drift(signal):\n    coeff = pywt.wavedec(signal, 'db6', level=9)\n    coeff[0] = np.zeros_like(coeff[0])\n    return pywt.waverec(coeff, 'db6')\n\ndef bandpass_filter(signal, low_freq=0.5, high_freq=40, fs=500, order=5):\n    nyquist = 0.5 * fs\n    low = low_freq / nyquist\n    high = high_freq / nyquist\n    b, a = butter(order, [low, high], btype='band')\n    return filtfilt(b, a, signal)\n\ndef normalize_and_segment(signal, r_peaks, window_size=256):\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    signal_normalized = scaler.fit_transform(signal.reshape(-1, 1)).flatten()\n    segments = []\n    half_window = window_size // 2\n    for r_peak in r_peaks:\n        start = max(r_peak - half_window, 0)\n        end = min(r_peak + half_window, len(signal_normalized))\n        if end - start == window_size:\n            segments.append(signal_normalized[start:end])\n    return np.array(segments)\n\n# Transformer Encoder 모듈\nclass TransformerEncoder(nn.Module):\n    def __init__(self, feature_size, num_heads, num_layers, dropout_rate):\n        super(TransformerEncoder, self).__init__()\n        encoder_layers = nn.TransformerEncoderLayer(\n            d_model=feature_size,\n            nhead=num_heads,\n            dropout=dropout_rate,\n            batch_first=True  # batch_first=True를 설정하여 배치 크기가 첫 번째 차원이 되도록 함\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n\n    def forward(self, src):\n        output = self.transformer_encoder(src)\n        return output\n\n# 하이브리드 CNN-GRU-Transformer 모델 정의\nclass HybridCNNGRUTransformer(nn.Module):\n    def __init__(self, num_classes):\n        super(HybridCNNGRUTransformer, self).__init__()\n        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, stride=1, padding=2)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.gru = nn.GRU(64, 100, batch_first=True, num_layers=2, dropout=0.5, bidirectional=True)  # GRU로 변경\n        self.transformer = TransformerEncoder(feature_size=200, num_heads=4, num_layers=2, dropout_rate=0.1)\n        self.fc = nn.Linear(200, num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = x.permute(0, 2, 1)  # Reshape for GRU\n        x, _ = self.gru(x)\n        x = self.transformer(x)\n        x = self.fc(x[:, -1, :])  # Use the last timestep\n        return x\n\n# 데이터 준비 및 DataLoader 설정\nbase_path = '/kaggle/input/ecgid-database'\nsegments, labels = load_and_preprocess_ecg_data(base_path)\nif len(segments) == 0 or len(labels) == 0:\n    raise ValueError(\"No data found. Please check the data path and files.\")\n\nsegments_train, segments_test, labels_train, labels_test = train_test_split(segments, labels, test_size=0.2, random_state=42)\n\nsegments_train_tensor = torch.tensor(segments_train, dtype=torch.float32).unsqueeze(1)\nsegments_test_tensor = torch.tensor(segments_test, dtype=torch.float32).unsqueeze(1)\nlabels_train_tensor = torch.tensor(labels_train, dtype=torch.long)\nlabels_test_tensor = torch.tensor(labels_test, dtype=torch.long)\n\ntrain_dataset = TensorDataset(segments_train_tensor, labels_train_tensor)\ntest_dataset = TensorDataset(segments_test_tensor, labels_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Training parameters\nnum_classes = np.unique(labels).size\nmodel = HybridCNNGRUTransformer(num_classes=num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n\n# Training and evaluation loop\ndef train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, scheduler, num_epochs=100):\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for data, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        scheduler.step()\n\n        print(f'Epoch {epoch+1}, Average Training Loss: {total_loss / len(train_loader)}')\n\n        model.eval()\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for data, labels in test_loader:\n                outputs = model(data)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n            print(f'Accuracy on Test Set: {100 * correct / total}%')\n\n# Train and evaluate the model\ntrain_and_evaluate(model, train_loader, test_loader, optimizer, criterion, scheduler)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-25T05:04:27.814679Z","iopub.execute_input":"2024-05-25T05:04:27.815052Z","iopub.status.idle":"2024-05-25T05:43:02.498105Z","shell.execute_reply.started":"2024-05-25T05:04:27.815020Z","shell.execute_reply":"2024-05-25T05:43:02.496702Z"},"trusted":true},"execution_count":null,"outputs":[]}]}