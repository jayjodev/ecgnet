{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3531470,"sourceType":"datasetVersion","datasetId":2124270}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport wfdb\nimport pywt\nfrom scipy.signal import butter, filtfilt\nfrom torch.optim.lr_scheduler import StepLR\n\n# 데이터 로딩 및 전처리 함수\ndef load_all_ecg_data(base_path):\n    all_segments = []\n    all_labels = []\n\n    for person_id in range(1, 90):  # Person_01부터 Person_89까지\n        person_path = os.path.join(base_path, f'Person_{person_id:02}')\n        for filename in os.listdir(person_path):\n            if filename.endswith('.hea') and not filename.startswith('.'):\n                record_name = filename.split('.')[0]\n                record_path = os.path.join(person_path, record_name)\n                try:\n                    record = wfdb.rdrecord(record_path)\n                    annotation = wfdb.rdann(record_path, 'atr')\n                    ecg_signal = record.p_signal[:, 0]\n                    ecg_signal = remove_baseline_drift(ecg_signal)\n                    ecg_signal = bandpass_filter(ecg_signal)\n                    segments = normalize_and_segment(ecg_signal, annotation.sample)\n                    all_segments.extend(segments)\n                    all_labels.extend([person_id - 1] * len(segments))\n                except FileNotFoundError:\n                    print(f\"File not found: {record_path}\")\n                    continue\n\n    return np.array(all_segments), np.array(all_labels)\n\ndef remove_baseline_drift(signal):\n    coeff = pywt.wavedec(signal, 'db6', level=9)\n    coeff[0] = np.zeros_like(coeff[0])\n    return pywt.waverec(coeff, 'db6')\n\ndef bandpass_filter(signal, low_freq=0.5, high_freq=40, fs=500, order=5):\n    nyquist = 0.5 * fs\n    low = low_freq / nyquist\n    high = high_freq / nyquist\n    b, a = butter(order, [low, high], btype='band')\n    return filtfilt(b, a, signal)\n\ndef normalize_and_segment(signal, r_peaks, window_size=180):\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    signal_normalized = scaler.fit_transform(signal.reshape(-1, 1)).flatten()\n    segments = []\n    half_window = window_size // 2\n    for r_peak in r_peaks:\n        start = max(r_peak - half_window, 0)\n        end = min(r_peak + half_window, len(signal_normalized))\n        if end - start == window_size:\n            segments.append(signal_normalized[start:end])\n    return np.array(segments)\n\n# 복잡한 CNN 모델 정의\nclass ComplexCNNModel(nn.Module):\n    def __init__(self, num_classes):\n        super(ComplexCNNModel, self).__init__()\n        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=2)\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(256 * 11, 256)\n        self.fc2 = nn.Linear(256, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.pool(F.relu(self.conv4(x)))\n        x = x.view(-1, 256 * 11)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# 데이터 준비 및 DataLoader 설정\nbase_path = '/kaggle/input/ecgid-database'\nsegments, labels = load_all_ecg_data(base_path)\n\n# 데이터 분할 및 PyTorch 텐서로 변환\nsegments_train, segments_test, labels_train, labels_test = train_test_split(segments, labels, test_size=0.2, random_state=42)\nsegments_train_tensor = torch.tensor(segments_train, dtype=torch.float32).unsqueeze(1)\nsegments_test_tensor = torch.tensor(segments_test, dtype=torch.float32).unsqueeze(1)\nlabels_train_tensor = torch.tensor(labels_train, dtype=torch.long)\nlabels_test_tensor = torch.tensor(labels_test, dtype=torch.long)\n\ntrain_dataset = TensorDataset(segments_train_tensor, labels_train_tensor)\ntest_dataset = TensorDataset(segments_test_tensor, labels_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# 학습 파라미터 설정\nnum_classes = np.unique(labels).size\nmodel = ComplexCNNModel(num_classes=num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n\n# 학습 및 평가 루프\ndef train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, scheduler, num_epochs=50):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for data, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        scheduler.step()\n        print(f'Epoch {epoch+1}, Average Training Loss: {total_loss / len(train_loader)}')\n\n        # 평가 단계\n        model.eval()\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for data, labels in test_loader:\n                outputs = model(data)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        accuracy = 100 * correct / total\n        print(f'Accuracy on Test Set: {accuracy}%')\n        if accuracy >= 90:\n            print(\"Reached target accuracy. Stopping training.\")\n            break\n\n# 모델 학습 및 평가\ntrain_and_evaluate(model, train_loader, test_loader, optimizer, criterion, scheduler)\n","metadata":{},"execution_count":null,"outputs":[]}]}